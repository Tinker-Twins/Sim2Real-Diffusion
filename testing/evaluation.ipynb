{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cpu with CUDA None (you have 2.5.1+cu118)\n",
      "    Python  3.10.16 (you have 3.10.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from diffusers.utils import load_image\n",
    "from transformers import (\n",
    "    CLIPTokenizer,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPImageProcessor,\n",
    ")\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory where the script is located\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Folder paths to load images\n",
    "image_folder = os.path.join(script_dir, \"input_autodrive_small_onroad\") # Load input frames from here\n",
    "style_folder = os.path.join(script_dir, \"style_autodrive_small_onroad\") # Load style frames from here\n",
    "output_folder = os.path.join(script_dir, \"output_autodrive_small_onroad\") # Load output frames from here\n",
    "\n",
    "# List all image files\n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "style_files = [f for f in os.listdir(style_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "output_files = [f for f in os.listdir(output_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityScores(nn.Module):\n",
    "    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_processor = image_processor\n",
    "        self.image_encoder = image_encoder\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        image = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        return {\"pixel_values\": image.to(\"cuda\")}\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\"input_ids\": inputs.input_ids.to(\"cuda\")}\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        preprocessed_image = self.preprocess_image(image)\n",
    "        image_features = self.image_encoder(**preprocessed_image).image_embeds\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        tokenized_text = self.tokenize_text(text)\n",
    "        text_features = self.text_encoder(**tokenized_text).text_embeds\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):\n",
    "        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)\n",
    "        return sim_direction\n",
    "    \n",
    "    def compute_cosine_similarity(self, img_feat_one, img_feat_two):\n",
    "        sim_cosine = F.cosine_similarity(img_feat_two, img_feat_one)\n",
    "        return sim_cosine\n",
    "\n",
    "    def forward(self, image_one, image_two, caption_one, caption_two):\n",
    "        img_feat_one = self.encode_image(image_one)\n",
    "        img_feat_two = self.encode_image(image_two)\n",
    "        text_feat_one = self.encode_text(caption_one)\n",
    "        text_feat_two = self.encode_text(caption_two)\n",
    "        directional_similarity = self.compute_directional_similarity(\n",
    "            img_feat_one, img_feat_two, text_feat_one, text_feat_two\n",
    "        )\n",
    "        cosine_similarity = self.compute_cosine_similarity(\n",
    "            img_feat_one, img_feat_two\n",
    "        )\n",
    "        return directional_similarity, cosine_similarity\n",
    "\n",
    "clip_id = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_id)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(\"cuda\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_id)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(\"cuda\")\n",
    "\n",
    "get_similarity_scores = SimilarityScores(tokenizer, text_encoder, image_processor, image_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LPIPS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "lpips_model = lpips.LPIPS(net='alex').to(\"cuda\")\n",
    "lpips_transform = T.Compose([\n",
    "    T.Resize((256, 256)), T.ToTensor(), # 256x256 tensor\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # [-1, 1] range\n",
    "])\n",
    "\n",
    "def get_lpips_score(original_image, edited_image):\n",
    "    return lpips_model(lpips_transform(original_image).unsqueeze(0).to(\"cuda\"),\n",
    "                       lpips_transform(edited_image).unsqueeze(0).to(\"cuda\")).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Style Difference Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsamak\\AppData\\Local\\anaconda3\\envs\\diffusers\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg_transform = T.Compose([\n",
    "    T.Resize((256, 256)), T.ToTensor(), # 256x256 tensor\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]) # VGG norm range\n",
    "])\n",
    "vgg = vgg19(pretrained=True).to(\"cuda\").eval()\n",
    "layers = {'features.0': 'conv1_1', 'features.5': 'conv2_1', 'features.10': 'conv3_1'}\n",
    "extractor = create_feature_extractor(vgg, return_nodes=layers)\n",
    "\n",
    "def gram_matrix(feat):\n",
    "    (b, c, h, w) = feat.size()\n",
    "    feat = feat.view(b, c, h * w)\n",
    "    G = torch.bmm(feat, feat.transpose(1, 2)) / (c * h * w)\n",
    "    return G\n",
    "\n",
    "def get_style_difference(original_image, style_image, edited_image):\n",
    "    in_feats = extractor(vgg_transform(original_image).unsqueeze(0).to(\"cuda\"))\n",
    "    out_feats = extractor(vgg_transform(edited_image).unsqueeze(0).to(\"cuda\"))\n",
    "    style_feats = extractor(vgg_transform(style_image).unsqueeze(0).to(\"cuda\"))\n",
    "    o2s_gram_loss = 0\n",
    "    i2s_gram_loss = 0\n",
    "    for layer in layers.values():\n",
    "        G_in = gram_matrix(in_feats[layer])\n",
    "        G_out = gram_matrix(out_feats[layer])\n",
    "        G_style = gram_matrix(style_feats[layer])\n",
    "        o2s_gram_loss += torch.mean((G_out - G_style) ** 2).item()\n",
    "        i2s_gram_loss += torch.mean((G_in - G_style) ** 2).item()\n",
    "    return o2s_gram_loss, i2s_gram_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics Evaulation on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↑ CLIP Directional Similarity:\t\t best=2.09e-01 | μ=1.08e-01 | σ=3.47e-02\n",
      "↑ Cosine Similarity (Input-to-Output):\t best=9.01e-01 | μ=8.15e-01 | σ=4.06e-02\n",
      "↓ LPIP Similarity (Input-to-Output):\t best=2.42e-01 | μ=4.06e-01 | σ=7.22e-02\n",
      "↓ Style Difference (Output-to-Style):\t best=6.32e-04 | μ=7.89e-04 | σ=1.15e-04\n",
      "↓ Style Difference (Input-to-Style):\t best=1.05e-03 | μ=1.32e-03 | σ=1.31e-04\n",
      "↑ Reduction in Style Difference:\t 40.33%\n"
     ]
    }
   ],
   "source": [
    "clipds_scores = []\n",
    "cosine_scores = []\n",
    "lpips_scores = []\n",
    "o2s_style_scores = []\n",
    "i2s_style_scores = []\n",
    "\n",
    "for i in range(len(image_files)):\n",
    "    original_image = load_image(os.path.join(image_folder, image_files[i]))\n",
    "    style_image = load_image(os.path.join(style_folder, style_files[0]))\n",
    "    edited_image = load_image(os.path.join(output_folder, output_files[i]))\n",
    "    original_caption = \"a black road on green surface in real world\"\n",
    "    modified_caption = \"a black road on green surface in simulation\"\n",
    "    clipds_score, cosine_score = get_similarity_scores(original_image, edited_image, original_caption, modified_caption)\n",
    "    lpips_score = get_lpips_score(original_image, edited_image)\n",
    "    o2s_gram_loss, i2s_gram_loss = get_style_difference(original_image, style_image, edited_image)\n",
    "    clipds_scores.append(float(clipds_score.detach().cpu()))\n",
    "    cosine_scores.append(float(cosine_score.detach().cpu()))\n",
    "    lpips_scores.append(lpips_score)\n",
    "    o2s_style_scores.append(o2s_gram_loss)\n",
    "    i2s_style_scores.append(i2s_gram_loss)\n",
    "\n",
    "print(f\"↑ CLIP Directional Similarity:\\t\\t best={np.max(clipds_scores):.2e} | μ={np.mean(clipds_scores):.2e} | σ={np.std(clipds_scores):.2e}\")\n",
    "print(f\"↑ Cosine Similarity (Input-to-Output):\\t best={np.max(cosine_scores):.2e} | μ={np.mean(cosine_scores):.2e} | σ={np.std(cosine_scores):.2e}\")\n",
    "print(f\"↓ LPIP Similarity (Input-to-Output):\\t best={np.min(lpips_scores):.2e} | μ={np.mean(lpips_scores):.2e} | σ={np.std(lpips_scores):.2e}\")\n",
    "print(f\"↓ Style Difference (Output-to-Style):\\t best={np.min(o2s_style_scores):.2e} | μ={np.mean(o2s_style_scores):.2e} | σ={np.std(o2s_style_scores):.2e}\")\n",
    "print(f\"↓ Style Difference (Input-to-Style):\\t best={np.min(i2s_style_scores):.2e} | μ={np.mean(i2s_style_scores):.2e} | σ={np.std(i2s_style_scores):.2e}\")\n",
    "print(f\"↑ Reduction in Style Difference:\\t {-100*(np.mean(o2s_style_scores)-np.mean(i2s_style_scores))/np.mean(i2s_style_scores):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
